\documentclass[a4j,11pt]{jarticle}
\usepackage{url}
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}
\usepackage[ipaex]{pxchfon}
\usepackage{listings}
\usepackage{plistings}
\usepackage{color}
\usepackage{amsmath, amssymb}
\usepackage{type1cm}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
	language={c},
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}



\setlength{\textwidth}{1.1\textwidth}
\setlength{\oddsidemargin}{-3pt}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{10mm}
\setlength{\headheight}{0mm}
\setlength{\headsep}{0mm}

\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits}

\begin{document}

\begin{center}
%\noindent
　\vspace{10mm}

{\bf {\huge 先端機械学習　後半課題}}
%\end{center}

\vspace{80mm}

提出日：2021年 8月10日

\vspace{10mm}

情報工学系

\vspace{10mm}

学籍番号：18B14822

\vspace{10mm}


\vspace{20mm}

{\bf {\LARGE 氏名：宮崎　直哉}}
\end{center}





\newpage




\section{問題1}

\subsection*{パラメータbの値：}

\begin{equation*}
    b = (0, -1000, -1000, -2000, -2000, -3000)
\end{equation*}

\subsection*{パラメータbを求める方法：}
本問題では$w_i = 1000 (i = 1,2,3,4,5,6)$である。この時、シグモイド関数は下図のようになる。この時、このシグモイド関数はステップ関数と考えることができる。

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=10cm]{p1-3.png}
\end{figure}

この時、$g_i = v_i\sigma(1000x + b_i)$という式の意味を考えてみる。
すると、シグモイド関数はxから[0,1]区間への写像であるので、$v_i$はステップ関数の高さを表すことがわかる。また、
\begin{eqnarray*}
    g_i &=& v_i\sigma(1000x + b_i)\\
    &=& v_i\sigma\left( 1000(x + \frac{b_i}{1000}) \right))
\end{eqnarray*}
と変形すると、$x = -\frac{b_i}{1000}$がステップ関数の変換点となっていることがわかる。例えば、
\begin{equation*}
    f(x) = 
    \left\{
        \begin{array}{ll}
            0 & (x < 3) \\
            5 & (3 \leq x)
        \end{array}
    \right.
\end{equation*}
となるようなステップ関数をシグモイド関数で近似することを考えると、変換点が$x = -\frac{b_i}{1000} = 3$で高さ$v_i=5で$あるので、

\begin{equation*}
    g = 5\sigma(1000x - 3000)
\end{equation*}

とすれば、下図のようにステップ関数をシグモイド関数で近似したグラフを意図して得られる。
\begin{figure}[hbtp]
    \centering
    \includegraphics[width=10cm]{p1-4.png}
\end{figure}

\newpage

この性質を利用して本問題を考えてみる。$f(x)$は以下のグラフのようである。
\begin{figure}[hbtp]
    \centering
    \includegraphics[width=10cm]{p1-2.png}
\end{figure}

式$g(x)$はシグモイド関数の和で表現されているので、上記の性質を合成した関数gを作成することができる。
ここで考えることは、ステップ関数の変換点と高さである。変換点として考えるべきなのは$x = 0,1,2,3$である。高さの変化については、$x=0:+2$、$x=1:-3$、$x=2:+4$、$x=3:-3$となっている。
高さについては、パラメータvの値を見ていくと、$x=0$において高さ$v_0 = 2$,$x=1$において高さ$v_1 = -2,v_2 = -1$,$x=2$において高さ$v_2 = 1,v_3 = 3$,$x=3$において高さ$v_6 = -3$のステップ関数をそれぞれ適用することで、$||f(x) - g(x)|| < \epsilon$となるようなパラメータbを決定することができる。

(本問題で作成したソースコードは\url{https://github.com/naoyaaan/AdvancedMachineLearning/blob/main/Okazaki/final.ipynb}にアップロードしています。)
\newpage
\section{問題2}

\subsection*{(1)}
\begin{equation*}
    h = 
    \begin{pmatrix}
        0 \\
        1.5
    \end{pmatrix}
\end{equation*}

\begin{equation*}
    \hat{y} = 0.5
\end{equation*}

\subsection*{(2)}

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=10cm]{p2-1.png}
\end{figure}

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=9cm]{p2-2.png}
\end{figure}

(本問題で作成したソースコードは\url{https://github.com/naoyaaan/AdvancedMachineLearning/blob/main/Okazaki/final.ipynb}にアップロードしています。)

\newpage
\section{問題3}

\subsection{WordSim-353}
\subsubsection*{評価に用いられるタスクの概要}
\textbf{WordSim-353(WS353)}は単語の類似性または関連性を測るためのデータセットである。
Word Similarity Task(単語の類似性を測るタスク)では、モデルによる単語間の類似性の評価と人間による評価の相関を計測する。

\subsubsection*{データセットの統計情報}
データセットの統計情報は以下の表のようである。

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=10cm]{p3-1.png}
\end{figure}

pairsは類似性を測る二つの単語を、ratersは評価した人間の人数,scaleは類似性のスコアを表している。

\subsubsection*{評価の尺度}
上記のタスクによる評価には Spearman’s rank correlation coefficientの値($\rho$×100)が用いられる。
$\rho$は[-1,1]の値をとり、この値が大きいほど、人間の評価とモデルの評価に相関があることを表している。
以下の表は実際にこのデータセットを用いてモデルを評価したときの結果である。

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=10cm]{p3-2.png}
\end{figure}

\newpage
\subsection{IMDb}
\subsubsection*{評価に用いられるタスクの概要}
\textbf{IMDb}はテキスト分類のタスクを評価するために用いられる。具体的には、Sentiment Analysis, Topic Detection, Language Detectionなどのタスクがある。
Sentiment Analysisは、 テキストがポジティブかネガティブがどちらでもないかを測るタスクである。Topic Detectionは、自動的にそのテキスト、文章のテーマ、トピックを特定するタスクである。Language Detectionは、与えられたテキストの言語を決定するタスクである。

\subsubsection*{データセットの統計情報}
IMDbはthe Internete Movie Databaseというサイトにおける、ポジティブ、ネガティブをラベル付けされた50000件のレビューからなるデータセットである。
ポジティブとネガティブのレビューが同じ数だけ収録されている。10段階の評価で4以下がネガティブ、7以上がポジティブと判断している。

\subsubsection*{評価の尺度}
IMDbを用いたSentiment Analysisでは、モデルのポジティブorネガティブの正答率(\%)を用いて評価している。
以下の表は、現在のIMDbにおけるモデルの予測精度のランキングである。
\begin{figure}
    \centering
    \includegraphics[width=10cm]{p3-3.png}
\end{figure}

\newpage
\section{問題4}
系列変換タスクにおいて、Transformerが再帰型ニューラルネットワーク(RNN)よりも優れている点として以下の2つが挙げられる。

1. 計算時間が早い

2. 並列化が可能

まずTransformerでは下図のような構造を用いている。

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=5cm]{p4-2.png}
\end{figure}

この中でもTransformerにおいてはSelf-Attentionをベースとした構造となっている。
RNNをベースとした構造と比較したのが下表である。

\begin{figure}[hbtp]
    \centering
    \includegraphics[width=12cm]{p4-1.png}
\end{figure}

これによると、RNNのレイヤーでは、$O(n^2\cdot d)$の計算量となっているが、Self-Attentionでは$O(n \cdot d^2)$となっている。
これだけではRNNがTransfomerより計算量が早いことはわからない。しかし、n(sequence length)やd(representation dimension)のサイズに注目してみると、たいていの場合$n < d$とnよりもdのほうがはるかに大きくなっている。そのため、Self-Attentionのレイヤーのほうが計算量が少ないことがわかる。

また、RNNでは入力に対してステップを経て処理をするので、Maximium path lengthの計算量は$O(n)$であるのに対して、Self-Attentionは並列計算を行うので、Maximium path lengthの計算量は$O(1)$となっている。GPU演算を行うことを考えるとSelf-Attentionは並列化できることというのが計算量の削減につなっており、RNNの構造よりも優れた点であるといえる。




\newpage
\section{問題5}


\begin{thebibliography}{99}
    \bibitem{fasttext} Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov. Enriching Word Vectors with Subword Information. arXiv:1607.04606. 2017
    \bibitem{Recursive NN} Minh-Thang, Luong Richard Socher, Christopher D. Manning. Better Word Representations with Recursive Neural Networks for Morphology.
    \bibitem{NLP-progress} NLP-progress. \url{http://nlpprogress.com/english/sentiment_analysis.html}. 閲覧日：2021年8月3日
    \bibitem{Attention} Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762. 2017
    \bibitem{}
    \bibitem{}
    \bibitem{}
    \bibitem{}
    \bibitem{}
    \bibitem{}
    \bibitem{}

\end{thebibliography}

\end{document}